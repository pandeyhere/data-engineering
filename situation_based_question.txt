I am assuming we are interested in designing Batch Processing Pipeline

Process Involved for designing the pipeline would be as follows:

Extract > Transform > Load

The folloing components (AWS managed Services) might possibly be used:

Kinesis Streaming Agent -> Kinesis Data Firehose ->  Transform Records -> Save Intermediate records in S3 -> AWS Lamdba -> AWS Redshift 
                                        |
                                        |->   AWS Lambda (Save source data in S3)


- Kinesis Streaming Agent/Producer Library on the website pulling data from database & pushing to Kinesis Streams/Data Firehose [Extract]
- Kinesis Data Firehose - [Transform]
    - AWS Lambda (Save source data in S3) 
    - Kinesis Data Firehose - Transform Records  
    - Kinesis Data Firehose - Save Intermediate records in S3 
    - AWS Lambda - Load Transformed Records in AWS Redshift 
- Data base/ Blob Storage - AWS Redshift - [Load]]
- Analytic Tools - to provide an interface/ option to generate reports by providing queries to view analytics data and derive insights